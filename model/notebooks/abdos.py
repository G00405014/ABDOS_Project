# -*- coding: utf-8 -*-
"""Abdosproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RzZgboYYND1nWIyWpFjVBdgQWQCXAl0H
"""

!pip install --upgrade tensorflow-text

# Cell 1: Environment Setup
from google.colab import drive
drive.mount('/content/drive')

!python --version
!pip install tensorflow pandas scikit-learn matplotlib opencv-python-headless tensorflowjs seaborn

# Cell 2: Imports and Paths
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import cv2
import logging

import tensorflow as tf
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Input, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

# For transfer learning
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.applications.efficientnet import preprocess_input as effnet_preprocess

from sklearn.preprocessing import LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report, confusion_matrix

# Paths (Adjust these as needed)
metadata_path = '/content/drive/MyDrive/Dataset/HAM10000_metadata.csv'
image_dir_1 = '/content/drive/MyDrive/Dataset/extracted_part_1'
image_dir_2 = '/content/drive/MyDrive/Dataset/extracted_part_2'
checkpoint_path = '/content/drive/MyDrive/skin_cancer_model.keras'
tflite_model_path = '/content/drive/MyDrive/skin_cancer_model.tflite'
tfjs_output_path = '/content/drive/MyDrive/skin_cancer_model_tfjs'

# Cell 3: Load Metadata and Encode Labels
df = pd.read_csv(metadata_path)

# Encode labels
label_encoder = LabelEncoder()
df['encoded_labels'] = label_encoder.fit_transform(df['dx'])
num_classes = len(label_encoder.classes_)
df['one_hot_labels'] = list(to_categorical(df['encoded_labels'], num_classes=num_classes))
print("Classes:", label_encoder.classes_)

def get_image_path(image_id):
    path1 = os.path.join(image_dir_1, f"{image_id}.jpg")
    path2 = os.path.join(image_dir_2, f"{image_id}.jpg")
    if os.path.exists(path1):
        return path1
    elif os.path.exists(path2):
        return path2
    return None

df['image_path'] = df['image_id'].apply(get_image_path)
df = df[df['image_path'].notnull()]  # Remove rows where images do not exist
print(f"Total valid images: {len(df)}")

# Cell 4: Split into Train/Val/Test
train_df = df.sample(frac=0.7, random_state=42)
temp_df = df.drop(train_df.index)
val_df = temp_df.sample(frac=0.5, random_state=42)
test_df = temp_df.drop(val_df.index)

print(f"Training samples: {len(train_df)}")
print(f"Validation samples: {len(val_df)}")
print(f"Testing samples: {len(test_df)}")

# Cell 5: Compute Class Weights
class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_df['encoded_labels']),
    y=train_df['encoded_labels']
)
class_weights = dict(enumerate(class_weights_array))
print("Class weights:", class_weights)

# Cell 6: Dataset Preparation and Augmentation
def process_image(image_path, label):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, [224, 224])
    img = img / 255.0
    return img, label

def prepare_dataset(dataframe, shuffle=True, batch_size=32):
    image_paths = dataframe['image_path'].values
    labels = dataframe['encoded_labels'].values
    ds = tf.data.Dataset.from_tensor_slices((image_paths, labels))
    ds = ds.map(process_image, num_parallel_calls=tf.data.AUTOTUNE)
    if shuffle:
        ds = ds.shuffle(1000)
    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)
    return ds

train_dataset = prepare_dataset(train_df, shuffle=True)
val_dataset   = prepare_dataset(val_df,   shuffle=False)
test_dataset  = prepare_dataset(test_df,  shuffle=False)

# Data Augmentation Pipeline
augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.1),
    tf.keras.layers.RandomZoom(0.1)
])

# Apply augmentation only on training set
train_dataset = train_dataset.map(
    lambda x, y: (augmentation(x, training=True), y),
    num_parallel_calls=tf.data.AUTOTUNE
)

# Cell 7: Build the Model using EfficientNetB0
def build_efficientnet_model(num_classes):
    # Load base model
    base_model = EfficientNetB0(
        weights='imagenet',
        include_top=False,
        input_shape=(224, 224, 3)
    )
    base_model.trainable = False  # Freeze base

    model = Sequential([
        Input(shape=(224, 224, 3)),
        tf.keras.layers.Lambda(effnet_preprocess),  # Preprocess input
        base_model,
        Flatten(),
        Dense(256, activation='relu'),
        Dropout(0.5),
        Dense(num_classes, activation='softmax')
    ])

    model.compile(
        optimizer=Adam(learning_rate=0.0005),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )

    return model

model = build_efficientnet_model(num_classes)
model.summary()

# Cell 8: Train the Model with Callbacks
callbacks = [
    ModelCheckpoint(checkpoint_path, save_best_only=True, monitor='val_loss', mode='min'),
    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),
    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
]

try:
    model = SkinCancerModel(num_classes=7)
    model.compile_model()
    model.train(train_dataset, val_dataset)
except Exception as e:
    logger.error(f"Training failed: {str(e)}")

# Cell 9: Evaluation - Load Best Model and Test
best_model = tf.keras.models.load_model(checkpoint_path, compile=False)
# Re-compile if necessary
best_model.compile(
    optimizer=Adam(learning_rate=0.0005),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

test_loss, test_acc = best_model.evaluate(test_dataset)
print(f"Test Accuracy: {test_acc:.4f}")
print(f"Test Loss: {test_loss:.4f}")

# Predictions for detailed metrics
y_true = []
y_pred = []

for images, labels in test_dataset:
    preds = best_model.predict(images)
    pred_classes = np.argmax(preds, axis=1)
    y_true.extend(labels.numpy())
    y_pred.extend(pred_classes)

y_true = np.array(y_true)
y_pred = np.array(y_pred)

# Cell 10: Classification Report and Confusion Matrix
print("Classification Report:")
print(classification_report(y_true, y_pred, target_names=label_encoder.classes_))

cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(
    cm, annot=True, fmt='d',
    xticklabels=label_encoder.classes_,
    yticklabels=label_encoder.classes_,
    cmap='Blues'
)
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

# Cell 11: Convert Model to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(best_model)
tflite_model = converter.convert()
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)
print("Model successfully converted to TensorFlow Lite!")

# Cell 12: Convert Model to TensorFlow.js
import tensorflowjs as tfjs
tfjs.converters.save_keras_model(best_model, tfjs_output_path)
print("Model successfully converted to TensorFlow.js!")

# Cell 13: Single Image Prediction Utility
def predict_image(image_path, model):
    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (224, 224))
    img = img.astype('float32') / 255.0
    img = np.expand_dims(img, axis=0)

    preds = model.predict(img)
    predicted_class = np.argmax(preds)
    confidence = np.max(preds)

    label_name = label_encoder.classes_[predicted_class]
    print(f"Predicted Class: {label_name}")
    print(f"Confidence: {confidence:.2f}")

    plt.imshow(cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB))
    plt.title(f"{label_name} ({confidence:.2f})")
    plt.axis('off')
    plt.show()

# Example: predict_image('/content/drive/MyDrive/test_image.jpg', best_model)

# Cell 14: Optional Fine-Tuning (Unfreeze Layers)
# If you want to fine-tune part (or all) of EfficientNet for better performance,
# you can unfreeze certain blocks of the base model here.

# base_model = best_model.layers[2]  # This is the EfficientNetB0 base in our Sequential
# base_model.trainable = True  # Unfreeze all (for demonstration)
# for layer in base_model.layers[:-20]:
#     layer.trainable = False  # Freeze early layers, unfreeze deeper layers

# Re-compile with a lower LR, then re-fit
# best_model.compile(
#     optimizer=Adam(learning_rate=1e-5),
#     loss='sparse_categorical_crossentropy',
#     metrics=['accuracy']
# )
# history_fine = best_model.fit(
#     train_dataset,
#     validation_data=val_dataset,
#     epochs=10,
#     callbacks=callbacks,
#     class_weight=class_weights
# )

# This step can further improve performance if you have enough data.